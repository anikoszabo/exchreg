\newcommand{\NWtarget}[2]{#2}
\newcommand{\NWlink}[2]{#2}
\newcommand{\NWtxtMacroDefBy}{Fragment defined by}
\newcommand{\NWtxtMacroRefIn}{Fragment referenced in}
\newcommand{\NWtxtMacroNoRef}{Fragment never referenced}
\newcommand{\NWtxtDefBy}{Defined by}
\newcommand{\NWtxtRefIn}{Referenced in}
\newcommand{\NWtxtNoRef}{Not referenced}
\newcommand{\NWtxtFileDefBy}{File defined by}
\newcommand{\NWtxtIdentsUsed}{Uses:}
\newcommand{\NWtxtIdentsNotUsed}{Never used}
\newcommand{\NWtxtIdentsDefed}{Defines:}
\newcommand{\NWsep}{${\diamond}$}
\newcommand{\NWnotglobal}{(not defined globally)}
\newcommand{\NWuseHyperlinks}{}
\documentclass[reqno]{amsart}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\renewcommand{\NWtarget}[2]{\hypertarget{#1}{#2}}
\renewcommand{\NWlink}[2]{\hyperlink{#1}{#2}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bpi}{\text{\boldmath $\pi$}}
\newcommand{\leqst}{\mathrel{\preceq^{st}}}
\newcommand{\geqst}{\mathrel{\succeq^{st}}}
\newcommand{\qz}{\boldsymbol{q}^{(0)}}
\DeclareMathOperator{\E}{\mathbb{E}}

\usepackage{dsfont}



\title{Semi-parametric generalized linear model for correlated binary data}
\author{Aniko Szabo}
\date{\today}
\begin{document}
\maketitle

Note: this setup assumes that parts defined in \texttt{SPregress.w} are available.

\section{Introduction}

We extend the Rathouz-Gao semi-parametric generalized linear model to clustered binary outcomes with varying cluster sizes. Treating the number of events $Y_i$ from a cluster scaled by corresponding cluster size $n_i$ as the response variable, we use a parametric regression model for the marginal response probability $Y_/n_i$ and assume that for the maximum cluster size $N$ the probability density distribution of $Y_i$ is an exponentially tilted version of the reference density distribution.

The conditional mean model is as follows:
\begin{equation}\label{M:conditionalMeanSPGLM}
     E(\dfrac{Y_i}{n_i} | \boldsymbol{Z}_i; \boldsymbol{\beta})=\mu(\boldsymbol{Z}_i,\boldsymbol{\beta}) \equiv \mu_i = h^{-1}( \boldsymbol{Z}_i^{T} \boldsymbol{\beta})
\end{equation}
where $\boldsymbol{Z}_i$ is a matrix of cluster-level covariates, the function $h(\cdot)$ is a known strictly increasing link function.

The assumed probability density function for a cluster of maximal size $N$ is:
\begin{equation}\label{M:densitySPGLM}
    q_{y,N} (\boldsymbol{Z}, \boldsymbol{\beta}) \propto q_{y,N}^{(0)} \times \exp [ - \omega(\boldsymbol{Z}, \boldsymbol{\beta}) \times y]
\end{equation}
where $q_{y,N}^{(0)}$ is a non-parametric reference distribution, and $\omega(\boldsymbol{Z}, \boldsymbol{\beta})$ is the tilting parameter chosen so that the expectation of $Y/N$ equals to the value defined by the mean model \eqref{M:conditionalMeanSPGLM}, i.e.\  $\sum_{y=0}^{N} \dfrac{y}{N} q_{y,N}(\boldsymbol{Z}, \boldsymbol{\beta}) = \mu = h^{-1}( \boldsymbol{Z}^{T} \boldsymbol{\beta}) $. 
%We define the tilting parameter for the reference density to be $0$, i.e., $\omega_0 \equiv 0$.

For the purpose of identifiability, the mean of marginal probability $\{ q_{y,N}^{(0)} \}$ is set at an arbitrary fixed value, which can be the mean of the reference group, or the overall marginal mean of the data. We extend the conditional mean and probability density model for  cluster sizes smaller than the maximal size $N$ by assuming marginal compatibility.

\section{Main function}

\begin{flushleft} \small\label{scrap1}\raggedright\small
\NWtarget{nuweb1}{} \verb@"../R/SPGLM.R"@\nobreak\ {\footnotesize {1}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@#' Fit semi-parametric GLM@\\
\mbox{}\verb@#'@\\
\mbox{}\verb@#'@{\tt @}\verb@rdname spglm@\\
\mbox{}\verb@#'@{\tt @}\verb@param formula a one-sided formula of the form \code{cbind(r, s) ~ predictors} where \code{r} and \code{s} give the number of responses and non-responses within each cluster, respectively (so the cluster size is \code{r+s}), and \code{predictors} describes the covariates.@\\
\mbox{}\verb@#'@{\tt @}\verb@param data  an optional matrix or data frame containing the variables in the formula \code{formula}. By default the variables are taken from \code{environment(formula).}@\\
\mbox{}\verb@#'@{\tt @}\verb@param subset  an optional vector specifying a subset of observations to be used.@\\
\mbox{}\verb@#'@{\tt @}\verb@param weights an optional vector specifying observation weights.@\\
\mbox{}\verb@#'@{\tt @}\verb@param offset  an optional vector specifying an offset term (predictor with a fixed coefficient of 1).@\\
\mbox{}\verb@#'@{\tt @}\verb@param link      a link function for the mean.@\\
\mbox{}\verb@#'@{\tt @}\verb@param mu0       an optional numeric value constraining the mean of the baseline distribution@\\
\mbox{}\verb@#'@{\tt @}\verb@param control a list with parameters controlling the algorithm.@\\
\mbox{}\verb@#'@{\tt @}\verb@return an object of class \code{spglm} with the fitted model.@\\
\mbox{}\verb@#'@{\tt @}\verb@export@\\
\mbox{}\verb@#' @{\tt @}\verb@importFrom stats terms model.matrix model.frame model.offset model.response model.weights@\\
\mbox{}\verb@@\\
\mbox{}\verb@spglm <- function(formula, data, subset, weights, offset, link="logit", mu0=NULL, @\\
\mbox{}\verb@                  control=list(eps=0.001, maxit=100)){@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Create model matrix from formula and data}\nobreak\ {\footnotesize \NWlink{nuweb2}{2}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    data_object <- list(model_matrix=mm, resp=Y, n=rowSums(Y), weights=weights, offset=offset, maxN=N, spt=0:N)@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Fit model}\nobreak\ {\footnotesize \NWlink{nuweb7}{7}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    mt <- attr(mf, "terms")@\\
\mbox{}\verb@    names(betas) <- colnames(mm)@\\
\mbox{}\verb@    names(referencef0) <- 0:N@\\
\mbox{}\verb@    rownames(vc) <- colnames(vc) <- c(names(betas), names(referencef0))@\\
\mbox{}\verb@    res <- list(coefficients = betas, f0=referencef0, vcov = vc, mu0=mu0, niter = iter, @\\
\mbox{}\verb@                loglik=llik, link = link, call = mc, terms = mt,@\\
\mbox{}\verb@                xlevels = .getXlevels(mt, mf),@\\
\mbox{}\verb@                data_object=data_object)@\\
\mbox{}\verb@    class(res) <- "spglm"@\\
\mbox{}\verb@    res@\\
\mbox{}\verb@@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtFileDefBy\ \NWlink{nuweb1}{1}\NWlink{nuweb3}{, 3}\NWlink{nuweb4}{, 4}\NWlink{nuweb5}{, 5}\NWlink{nuweb6}{, 6}\NWlink{nuweb?}{, ?}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\begin{flushleft} \small\label{scrap2}\raggedright\small
\NWtarget{nuweb2}{} $\langle\,${\itshape Create model matrix from formula and data}\nobreak\ {\footnotesize {2}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@   if (missing(formula) || (length(formula) != 3L))@\\
\mbox{}\verb@        stop("'formula' missing or incorrect")@\\
\mbox{}\verb@   if (missing(data))@\\
\mbox{}\verb@        data <- environment(formula)@\\
\mbox{}\verb@    mc <- match.call(expand.dots = FALSE)@\\
\mbox{}\verb@    m <- match(c("formula", "data", "subset", "weights", "offset"), names(mc), 0L)@\\
\mbox{}\verb@    m <- mc[c(1L,m)]@\\
\mbox{}\verb@    if (is.matrix(eval(m$data, parent.frame())))@\\
\mbox{}\verb@        m$data <- as.data.frame(data)@\\
\mbox{}\verb@    m[[1L]] <- quote(stats::model.frame)@\\
\mbox{}\verb@    mf <- eval(m, parent.frame())@\\
\mbox{}\verb@@\\
\mbox{}\verb@    # extract and check response variable@\\
\mbox{}\verb@    Y <- model.response(mf)@\\
\mbox{}\verb@    if (ncol(Y) != 2) stop("The response variable should be a 2-column matrix")@\\
\mbox{}\verb@@\\
\mbox{}\verb@    # create model matrix@\\
\mbox{}\verb@    mm <- model.matrix(formula, data=mf)@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    if (is.character(link)) {@\\
\mbox{}\verb@      link <- stats::make.link(link)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    else if (!is.list(link) || !(all(c("linkfun", "linkinv", @\\
\mbox{}\verb@                                     "mu.eta") %in% names(link)))) {@\\
\mbox{}\verb@      stop(paste0("link should be a character string or a list containing ", @\\
\mbox{}\verb@                "functions named linkfun, linkinv, and mu.eta"))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    # extract offset@\\
\mbox{}\verb@    offset <- as.vector(model.offset(mf))@\\
\mbox{}\verb@    if (is.null(offset)) @\\
\mbox{}\verb@      offset <- rep(0, nrow(mm))@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    # extract weights@\\
\mbox{}\verb@    weights <- as.vector(model.weights(mf))@\\
\mbox{}\verb@    if (!is.null(weights) && !is.numeric(weights))@\\
\mbox{}\verb@        stop("'weights' must be a numeric vector")@\\
\mbox{}\verb@    if (!is.null(weights) && any(weights < 0))@\\
\mbox{}\verb@        stop("negative weights not allowed")@\\
\mbox{}\verb@    if (is.null(weights))@\\
\mbox{}\verb@        weights <- rep(1, nrow(mm))@\\
\mbox{}\verb@   @\\
\mbox{}\verb@   # define dimensions     @\\
\mbox{}\verb@    N <- max(rowSums(Y))@\\
\mbox{}\verb@    nobs <- nrow(mm)@\\
\mbox{}\verb@    p <- ncol(mm)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb1}{1}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Methods for the \texttt{spglm} class}

First, define a printing method which does not show the saved data and model matrix

\begin{flushleft} \small\label{scrap3}\raggedright\small
\NWtarget{nuweb3}{} \verb@"../R/SPGLM.R"@\nobreak\ {\footnotesize {3}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@#' @{\tt @}\verb@export@\\
\mbox{}\verb@#' @{\tt @}\verb@importFrom stats printCoefmat@\\
\mbox{}\verb@print.spglm <- function(x, digits = max(3L, getOption("digits") - 3L),...){@\\
\mbox{}\verb@  # based on print.lm, summary.lm, and print.summary.lm@\\
\mbox{}\verb@  cat("\nA semi-parametric generalized linear regression model fit\n")@\\
\mbox{}\verb@  cat("\nCall:\n", paste(deparse(x$call), sep = "\n", collapse = "\n"),@\\
\mbox{}\verb@        "\n\n", sep = "")@\\
\mbox{}\verb@  if (length(x$coefficients)) {@\\
\mbox{}\verb@    beta <- x$coefficients@\\
\mbox{}\verb@    p <- length(beta)@\\
\mbox{}\verb@    SEbeta <- sqrt(diag(x$vcov[1:p, 1:p]))@\\
\mbox{}\verb@    z <- beta / SEbeta@\\
\mbox{}\verb@    pval <- 2 * stats::pnorm(abs(z), lower.tail=FALSE)@\\
\mbox{}\verb@    coefmat <-  cbind(Estimate = beta, `Std. Error` = SEbeta, @\\
\mbox{}\verb@                      `z value` = z, `Pr(>|z|)` = pval)@\\
\mbox{}\verb@    cat("Coefficients:\n")@\\
\mbox{}\verb@    printCoefmat(coefmat, digits = digits, na.print = "NA", ...)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    else cat("No coefficients\n")@\\
\mbox{}\verb@@\\
\mbox{}\verb@    if (length(x$f0)){@\\
\mbox{}\verb@        cat("\n Baseline probabilities (q0):\n")        @\\
\mbox{}\verb@        q0 <- x$f0@\\
\mbox{}\verb@         print.default(format(q0, digits = digits),@\\
\mbox{}\verb@            print.gap = 2, quote = FALSE)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    else cat("No baseline joint probabilities\n")@\\
\mbox{}\verb@@\\
\mbox{}\verb@   cat("\n Log-likelihood: ", format(x$loglik, digits=digits), "\n")@\\
\mbox{}\verb@   invisible(x)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtFileDefBy\ \NWlink{nuweb1}{1}\NWlink{nuweb3}{, 3}\NWlink{nuweb4}{, 4}\NWlink{nuweb5}{, 5}\NWlink{nuweb6}{, 6}\NWlink{nuweb?}{, ?}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
Custom extractors for the coefficients and variance-covariance matrix, with option to get only beta's (default), only the backbone pdf, or both.

\begin{flushleft} \small\label{scrap4}\raggedright\small
\NWtarget{nuweb4}{} \verb@"../R/SPGLM.R"@\nobreak\ {\footnotesize {4}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@#' Extracting components of fitted `spglm` object@\\
\mbox{}\verb@#'@\\
\mbox{}\verb@#' @{\tt @}\verb@param object fitted model of class \code{spglm}@\\
\mbox{}\verb@#' @{\tt @}\verb@param beta logical, whether the regression parameter estimates / variances should be returned@\\
\mbox{}\verb@#' @{\tt @}\verb@param f0 logical, whether the backbone density estimates / variances should be returned@\\
\mbox{}\verb@#' @{\tt @}\verb@param ... not used, present fo consistency with generics@\\
\mbox{}\verb@#' @{\tt @}\verb@importFrom stats coef@\\
\mbox{}\verb@#' @{\tt @}\verb@export@\\
\mbox{}\verb@@\\
\mbox{}\verb@coef.spglm <- function(object, beta=TRUE, f0=FALSE, ...){@\\
\mbox{}\verb@  res <- NULL@\\
\mbox{}\verb@  if (beta) res <- c(res, object$coefficients)@\\
\mbox{}\verb@  if (f0) res <- c(res, object$f0)@\\
\mbox{}\verb@  res@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@#' @{\tt @}\verb@rdname coef.spglm@\\
\mbox{}\verb@#'@\\
\mbox{}\verb@#' @{\tt @}\verb@importFrom stats vcov@\\
\mbox{}\verb@#' @{\tt @}\verb@export@\\
\mbox{}\verb@vcov.spglm <- function(object, beta=TRUE, f0=FALSE, ...){@\\
\mbox{}\verb@  p <- length(object$coefficients)@\\
\mbox{}\verb@  m <- nrow(object$vcov)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  idx <- NULL@\\
\mbox{}\verb@  if (beta) idx <- c(idx, 1:p)@\\
\mbox{}\verb@  if (f0) idx <- c(idx, (p+1):m)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  vc <- object$vcov[idx,idx]@\\
\mbox{}\verb@  vc@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@#' @{\tt @}\verb@rdname coef.spglm@\\
\mbox{}\verb@#'@\\
\mbox{}\verb@#' @{\tt @}\verb@importFrom stats logLik@\\
\mbox{}\verb@#' @{\tt @}\verb@export@\\
\mbox{}\verb@logLik.spglm <- function(object, ...){@\\
\mbox{}\verb@  object$loglik@\\
\mbox{}\verb@}@\\
\mbox{}\verb@                              @\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtFileDefBy\ \NWlink{nuweb1}{1}\NWlink{nuweb3}{, 3}\NWlink{nuweb4}{, 4}\NWlink{nuweb5}{, 5}\NWlink{nuweb6}{, 6}\NWlink{nuweb?}{, ?}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
The prediction method will predict for a variety of scenarios:
\begin{itemize}
\item Input data set:
  \begin{itemize}
    \item the data used in the fitting;
    \item new data.
  \end{itemize}
\item Results:
  \begin{itemize}
    \item $\mu(z,\beta)$: the mean event probability, given $z$;
    \item $p_{r,n}(z)$: the probability of observing the given $r$ responses with cluster size $n$, given $z$;
    \item $\{p_{\cdot,n}(z)\}$: the entire vector of response probabilities for cluster size $n$, given $z$ (will be a list due to varying lengths);
    \item $\beta'z$: the linear predictor value $z$;
    \item $\omega(\beta'z)$: the tilting parameter at predictor value $z$.
  \end{itemize}
\end{itemize}

\begin{flushleft} \small\label{scrap5}\raggedright\small
\NWtarget{nuweb5}{} \verb@"../R/SPGLM.R"@\nobreak\ {\footnotesize {5}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@#' Predict methods for SPGLM fits@\\
\mbox{}\verb@#'@\\
\mbox{}\verb@#' Obtains predictions from a fitted semi-parametric generalized linear model. Note that \code{offset}@\\
\mbox{}\verb@#' and \code{weight} terms are not implemented for predicting from new data.@\\
\mbox{}\verb@#' @{\tt @}\verb@param object fitted model of class \code{spglm}@\\
\mbox{}\verb@#' @{\tt @}\verb@param newdata optionally, a data frame in which to look for covariate values for prediction. @\\
\mbox{}\verb@#'  If NULL, the original data set is used@\\
\mbox{}\verb@#' @{\tt @}\verb@param type the type of prediction requested. The default is "mean", the mean event probability;@\\
\mbox{}\verb@#' "prob" requests the probability of the observation (given number of responses with given cluster size), @\\
\mbox{}\verb@#' "tilt" returns the tilting parameter which achieves the modeled mean from the baseline distribution;@\\
\mbox{}\verb@#' and "lp" requests the linear predictor.@\\
\mbox{}\verb@#' @{\tt @}\verb@param newn if \code{newdata} is provided and \code{type="prob"}, an integer or integer vector specifying the clustersize for the predictors@\\
\mbox{}\verb@#' @{\tt @}\verb@param newevents if \code{newdata} is provided and \code{type="prob"}, an integer or integer vector specifying the @\\
\mbox{}\verb@#'  number of events for the predictions@\\
\mbox{}\verb@#' @{\tt @}\verb@param ... not used, present for consistency with generic@\\
\mbox{}\verb@#' @{\tt @}\verb@export@\\
\mbox{}\verb@#' @{\tt @}\verb@importFrom stats predict .checkMFClasses .getXlevels delete.response@\\
\mbox{}\verb@@\\
\mbox{}\verb@predict.spglm <- function(object, newdata=NULL,@\\
\mbox{}\verb@                              type=c("mean", "prob", "tilt", "lp"),@\\
\mbox{}\verb@                              newn=NULL, newevents=NULL, ...){@\\
\mbox{}\verb@  type <- match.arg(type)@\\
\mbox{}\verb@  tt <- terms(object)@\\
\mbox{}\verb@  if (!missing(newdata)){    @\\
\mbox{}\verb@    Terms <- delete.response(tt)@\\
\mbox{}\verb@    m <- model.frame(Terms, newdata, xlev = object$xlevels)@\\
\mbox{}\verb@    if (!is.null(cl <- attr(Terms, "dataClasses")))@\\
\mbox{}\verb@            .checkMFClasses(cl, m)@\\
\mbox{}\verb@    mm <- model.matrix(Terms, m)@\\
\mbox{}\verb@    data_object <- list(model_matrix=mm, offset = rep(0, nrow(mm)), weights=rep(1, nrow(mm)),@\\
\mbox{}\verb@                        maxN = object$data_object$maxN, spt = object$data_object$spt)@\\
\mbox{}\verb@    if (!missing(newn)){@\\
\mbox{}\verb@       if (!(length(newn) == 1L || length(newn) == nrow(newdata)))@\\
\mbox{}\verb@          stop("'newn' should have length 1 or equal to the number of rows of 'newdata'")@\\
\mbox{}\verb@        data_object$n <- rep(newn, length=nrow(newdata))@\\
\mbox{}\verb@        if (!all(data_object$n %in% 0:data_object$maxN))@\\
\mbox{}\verb@          stop("Values in 'newn' should be integers between 0 and the maximum cluster size in the original data")@\\
\mbox{}\verb@       }@\\
\mbox{}\verb@    if (!missing(newevents)){@\\
\mbox{}\verb@       if (!(length(newevents) == 1L || length(newevents) == nrow(newdata)))@\\
\mbox{}\verb@          stop("'newevents' should have length 1 or equal to the number of rows of 'newdata'")@\\
\mbox{}\verb@        data_object$resp <- cbind(rep(newevents, length=nrow(newdata)))@\\
\mbox{}\verb@       }@\\
\mbox{}\verb@  } else {@\\
\mbox{}\verb@    data_object <- object$data_object@\\
\mbox{}\verb@  }@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  if (type=="mean"){@\\
\mbox{}\verb@    pred <- spglm_pred_mean(beta=object$coefficients, data_object, link=object$link)@\\
\mbox{}\verb@  } else@\\
\mbox{}\verb@  if (type=="prob"){@\\
\mbox{}\verb@    if (!missing(newdata) && (missing(newn) || missing(newevents)))@\\
\mbox{}\verb@       stop("For prediction of probability vectors with new data, cluster sizes should be specified in 'newn' and number of events in 'newevents'.")     @\\
\mbox{}\verb@     pred <- spglm_probs(beta=object$coefficients, f0=object$f0, @\\
\mbox{}\verb@                              data_object=data_object, link=object$link)@\\
\mbox{}\verb@  } else@\\
\mbox{}\verb@  if (type == "lp"){@\\
\mbox{}\verb@     pred <- spglm_lp(beta=object$coefficients, data_object=data_object)@\\
\mbox{}\verb@  } else@\\
\mbox{}\verb@  if (type == "tilt"){@\\
\mbox{}\verb@     pred <- spglm_tilt(beta=object$coefficients, f0=object$f0, @\\
\mbox{}\verb@                       data_object=data_object, link=object$link)@\\
\mbox{}\verb@  }@\\
\mbox{}\verb@  return(pred)    @\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtFileDefBy\ \NWlink{nuweb1}{1}\NWlink{nuweb3}{, 3}\NWlink{nuweb4}{, 4}\NWlink{nuweb5}{, 5}\NWlink{nuweb6}{, 6}\NWlink{nuweb?}{, ?}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\section{EM algorithm for model fitting}
We will combine an Expectation Maximization with the Newton-Raphson algorithm proposed by Rathouz and Gao (2009) to estimate the model parameters. Recall that the observed data consists of the number of events, cluster size, and cluster-level covariates: $(y_i, N_i, \boldsymbol{Z}_i)$, $i=1,\cdots,I$. 

\subsection{Observed data log-likelihood}
Under the marginal compatibility assumption, the observed log-likelihood function is:
\begin{equation}\label{E:loglikelihood}
\begin{split}
    \ell (q_{y,N}, \boldsymbol{\beta}) &= \sum_{i=1}^I \log \{ q_{y_i,N_i} (\boldsymbol{Z}_i, \boldsymbol{\beta}) \} \\
    &= \sum_{i=1}^I \log \bigg[ \binom{N_i}{y_i} \sum_{t=y}^{N-N_i+y_i} \dfrac{\binom{N-N_i}{t-y_i}}{\binom{N}{t}} \times \dfrac{q_{t,N}^{(0)} \times \exp \{ \omega(\boldsymbol{Z}_i; \boldsymbol{\beta}) t \}} {\sum_{t'=0}^{N} q_{t',N}^{(0)} \times \exp \{ \omega(\boldsymbol{Z}_i; \boldsymbol{\beta}) t' \}}  \bigg].
\end{split}
\end{equation}

\subsection{Model prediction and likelihood}
We define internal function to calculate predicted values and the log-likelihood.

\begin{flushleft} \small\label{scrap6}\raggedright\small
\NWtarget{nuweb6}{} \verb@"../R/SPGLM.R"@\nobreak\ {\footnotesize {6}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@#' @{\tt @}\verb@keywords internal@\\
\mbox{}\verb@#' @{\tt @}\verb@importFrom stats dhyper@\\
\mbox{}\verb@spglm_lp <- function(beta, data_object){@\\
\mbox{}\verb@  eta <- c(data_object$model_matrix %*% beta + data_object$offset)@\\
\mbox{}\verb@  eta@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@spglm_pred_mean <- function(beta, data_object, link){@\\
\mbox{}\verb@  eta <- c(data_object$model_matrix %*% beta + data_object$offset)@\\
\mbox{}\verb@  mu <- link$linkinv(eta)@\\
\mbox{}\verb@  mu@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@spglm_tilt <- function(beta, f0, data_object, link){@\\
\mbox{}\verb@@\\
\mbox{}\verb@    mu <- spglm_pred_mean(beta=beta, data_object=data_object, link=link)@\\
\mbox{}\verb@    N <- data_object$maxN@\\
\mbox{}\verb@    nobs <- nrow(data_object$model_matrix)@\\
\mbox{}\verb@    spt <- data_object$spt@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    # ySptIndex is only used to calculate the log-likelihood, which we will not be using@\\
\mbox{}\verb@    th <- getTheta(spt=spt/N, f0=f0[spt+1], mu=mu, weights=data_object$weights, ySptIndex=rep(1, nobs),@\\
\mbox{}\verb@                   thetaStart=NULL, thetaControl=theta.control())@\\
\mbox{}\verb@    th$theta@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@spglm_probs <- function(beta, f0, data_object, link){@\\
\mbox{}\verb@@\\
\mbox{}\verb@    mu <- spglm_pred_mean(beta=beta, data_object=data_object, link=link)@\\
\mbox{}\verb@    N <- data_object$maxN@\\
\mbox{}\verb@    nobs <- nrow(data_object$model_matrix)@\\
\mbox{}\verb@    spt <- data_object$spt@\\
\mbox{}\verb@   @\\
\mbox{}\verb@    # ySptIndex is only used to calculate the log-likelihood, which we will not be using@\\
\mbox{}\verb@    th <- getTheta(spt=spt/N, f0=f0[spt+1], mu=mu, weights=data_object$weights, ySptIndex=rep(1, nobs),@\\
\mbox{}\verb@                   thetaStart=NULL, thetaControl=theta.control())@\\
\mbox{}\verb@                   @\\
\mbox{}\verb@    hp <- sapply(0:N, function(t)dhyper(x=data_object$resp[,1], m=data_object$n, n=N-data_object$n, k=t))  @\\
\mbox{}\verb@    @\\
\mbox{}\verb@    # fill in 0's for values with no support@\\
\mbox{}\verb@    fTilt <- matrix(0, ncol = nobs, nrow = N+1)@\\
\mbox{}\verb@    fTilt[spt+1,] <- th$fTilt@\\
\mbox{}\verb@    #probs <- rowSums(t(th$fTilt) * hp)@\\
\mbox{}\verb@    probs <- rowSums(t(fTilt) * hp)@\\
\mbox{}\verb@    probs@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@spglm_loglik <- function(beta, f0, data_object, link){@\\
\mbox{}\verb@  @\\
\mbox{}\verb@    probs <- spglm_probs(beta, f0, data_object, link)@\\
\mbox{}\verb@    llik <- log(probs) %*% data_object$weights@\\
\mbox{}\verb@    c(llik)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtFileDefBy\ \NWlink{nuweb1}{1}\NWlink{nuweb3}{, 3}\NWlink{nuweb4}{, 4}\NWlink{nuweb5}{, 5}\NWlink{nuweb6}{, 6}\NWlink{nuweb?}{, ?}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
It is difficult to utilize the observed log-likelihood directly for parameter estimation. Its score function is complex because there is a logarithm of a sum in \eqref{E:loglikelihood}. Therefore, we implement an EM algorithm for parameter estimation.


\begin{flushleft} \small\label{scrap7}\raggedright\small
\NWtarget{nuweb7}{} $\langle\,${\itshape Fit model}\nobreak\ {\footnotesize {7}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@ @\hbox{$\langle\,${\itshape Set starting values}\nobreak\ {\footnotesize \NWlink{nuweb10a}{10a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@ @\hbox{$\langle\,${\itshape Set up for E-step}\nobreak\ {\footnotesize \NWlink{nuweb8}{8}}$\,\rangle$}\verb@@\\
\mbox{}\verb@ @\hbox{$\langle\,${\itshape Identify zero support}\nobreak\ {\footnotesize \NWlink{nuweb10b}{10b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@ iter <- 0@\\
\mbox{}\verb@ difference <- 100@\\
\mbox{}\verb@ while (iter < control$maxit & difference > control$eps) {@\\
\mbox{}\verb@    iter <- iter + 1@\\
\mbox{}\verb@    referencef0Pre <- referencef0@\\
\mbox{}\verb@    betasPre <- betas@\\
\mbox{}\verb@    llikPre <- llik@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape E-step}\nobreak\ {\footnotesize \NWlink{nuweb9a}{9a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape M-step}\nobreak\ {\footnotesize \NWlink{nuweb9b}{9b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    difference <- abs(llik - llikPre)@\\
\mbox{}\verb@  }@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  @\hbox{$\langle\,${\itshape Calculate standard errors}\nobreak\ {\footnotesize \NWlink{nuweb11}{11}}$\,\rangle$}\verb@@\\
\mbox{}\verb@  @\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb1}{1}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Missing data setup}
Stefanescu et al (2003) have proved that the marginal compatibility assumption is equivalent to assuming that each cluster of size $N'$ arises from a cluster of maximal cluster size $N$ with some binary observations missing completely missing at random (MCAR). The missing data setup and the expectation step from EM algorithm are based on this MCAR interpretation. 

For the setup of the EM algorithm, the complete data are clusters of size $N$ with a corresponding number of events $s_i$, where $s_i \in \{y_i, \cdots, N \}$. The missing data correspond to the unobserved outcomes in of the $(N - N_i)$ cluster components. The missing data can be summarized as $(s_i - y_i)$ events out of $(N - N_i)$ elements.

The complete data log-likelihood is:
\begin{equation}\label{E:logLikelihoodCompleteMapFromObs}
\begin{split}
    \ell_{\text{complete}} (q_{y,N}, \boldsymbol{\beta} \mid N_i,s_i) &= \sum_{i=0}^I \log \{ q_{s_i,N}(\boldsymbol{Z}_i; \boldsymbol{\beta}) \} \\
    &= \sum_{i=0}^I \sum_{s=y_i}^N {\mathds{1}}(s_i=s) \times \log \{ q_{s,N}(\boldsymbol{Z}_i; \boldsymbol{\beta}) \}, 
\end{split}
\end{equation}
where $q_{s_i,N}$ is the probability of achieving $s_i$ events in a cluster of size $N$ in the complete data set. The $\mathds{1}$ denotes the indicator function.


Equation \ref{E:logLikelihoodCompleteMapFromObs} can be interpreted as the log-likelihood of a model with fixed cluster size $N$ for an expanded data set: each observation with cluster size $N_i$ and number of responses $y_i$ is expanded to $(N+1)$ replicates with $0, 1, \ldots, N$, responses, respectively.

\begin{flushleft} \small\label{scrap8}\raggedright\small
\NWtarget{nuweb8}{} $\langle\,${\itshape Set up for E-step}\nobreak\ {\footnotesize {8}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  # replace each cluster with N+1 clusters of size N@\\
\mbox{}\verb@  new <- cbind(y=0:N)@\\
\mbox{}\verb@  rep_idx <- rep(1:nrow(Y), each=nrow(new))@\\
\mbox{}\verb@  Y2 <- cbind(1:nrow(Y), Y)[rep_idx,]@\\
\mbox{}\verb@  colnames(Y2) <- c("i", "resp","nonresp")@\\
\mbox{}\verb@@\\
\mbox{}\verb@  rep_idx2 <- rep(1:nrow(new), times=nrow(Y))@\\
\mbox{}\verb@  Ycomb <- cbind(Y2, new[rep_idx2, ,drop=FALSE])@\\
\mbox{}\verb@  # select possible combinations@\\
\mbox{}\verb@  possible <- (Ycomb[,"y"] >= Ycomb[,"resp"]) & (N-Ycomb[,"y"] >= Ycomb[,"nonresp"])@\\
\mbox{}\verb@  Ycomb <- Ycomb[possible,]@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  obs_start <- match(1:nobs, Ycomb[,"i"])@\\
\mbox{}\verb@ @\\
\mbox{}\verb@  mm2 <- mm[Ycomb[,"i"], ,drop=FALSE]@\\
\mbox{}\verb@  weights2 <- weights[Ycomb[,"i"]]@\\
\mbox{}\verb@  offset2 <- offset[Ycomb[,"i"]]@\\
\mbox{}\verb@@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Expectation step}

We take the expectation of complete data log-likelihood \eqref{E:logLikelihoodCompleteMapFromObs} given parameters from the previous $k^{th}$ iteration, which is shown as follows:
\begin{equation}\label{E:ExpectedlogLikelihoodCompleteMapFromObs}
\begin{split}
    Q \bigg( q_{y,N}^{(k)}, \boldsymbol{\beta}^{(k)} \bigg) &= 
     \E \bigg\{ \ell_{\text{complete}} (q_{y,N}, \boldsymbol{\beta} \mid N_i, s_i) \mid q_{y,N}^{(k)}, \boldsymbol{\beta}^{(k)}, N_i, y_i \bigg\} \\
    &= \E \Bigg[ \sum_{i=0}^I \sum_{s=0}^N {\mathds{1}}(s=s_i) \times \log \{ q_{s,N}(\boldsymbol{Z}_i; \boldsymbol{\beta}) \} \mid q_{y,N}^{(k)}, y_i,N_i \Bigg] \\
    &= \sum_{i=0}^I \sum_{s=0}^N \underbrace{ \E \big\{ {\mathds{1}}(s=s_i) \mid q_{y,N}^{(k)}, \boldsymbol{\beta}^{(k)}, y_i, N_i \big\}}_{p_{N_iy_is_i}^{(k)}} \times \log \{q_{s,N}(\boldsymbol{Z}_i; \boldsymbol{\beta}) \},
\end{split}
\end{equation}
where $p_{N'y'y} = \textbf{Pr}_N(Y=y \mid Y'=y',N')$ denotes the conditional probability of achieving $y$ events in the complete cluster of size $N$, given that $y'$ events have been observed in the original cluster of size $N'$. 
The expression for $p_{N'y'y}$ can be derived from $q_{y,N}$ using Bayes theorem and the MCAR interpretation as follows:
\begin{equation}\label{D:prst}
\begin{split}
p_{N'y'y} &= \textbf{Pr}_N(Y=y \mid Y'=y',N') \\ 
          &= \frac{\textbf{Pr}_{N'}(Y'=y' \mid Y=y) \textbf{Pr}_N(Y=y)}{\sum_{t=0}^{N} \textbf{Pr}_{y'}(Y'=y' \mid Y=t) \textbf{Pr}_N(Y=t)} \\ 
          &= \frac{\binom{y}{y'} \binom{N-y}{N'-y'} q_{y,N}}{\sum_{t=y'}^{N-N'+y'} \dbinom{t}{y'} \binom{N-t}{N'-y'} q_{t,N}},
\end{split}
\end{equation}
and $p_{N_iy_is_i}^{(k)}$ at the $k$th step of the iteration can be obtained by using $q_{y,N}^{(k)}$ on the right-hand side of the expression. 

The value of $q_{y,N}^{(k)}(\boldsymbol{Z}_i; \boldsymbol{\beta}^{(k)})$ can be obtained from ${q_{y,N}^{(0)}}^{(k)}$ using \ref{M:densitySPGLM}. But it is also returned by \texttt{gldrmFit} as \texttt{fTiltMatrix}.


\begin{flushleft} \small\label{scrap9}\raggedright\small
\NWtarget{nuweb9a}{} $\langle\,${\itshape E-step}\nobreak\ {\footnotesize {9a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  # convert fTiltMatrix to long vector, watching out for potentially different support@\\
\mbox{}\verb@  y_idx <- match(Ycomb[,"y"], 0:N)@\\
\mbox{}\verb@  fTilt2 <- ifelse(!is.na(y_idx), fTiltMatrix[cbind(Ycomb[,"i"], y_idx)], 0)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # numerator of weights@\\
\mbox{}\verb@  pp_num <- choose(n=Ycomb[,"y"], k=Ycomb[,"resp"]) * @\\
\mbox{}\verb@           choose(n=N-Ycomb[,"y"], k=Ycomb[,"nonresp"]) * @\\
\mbox{}\verb@           fTilt2@\\
\mbox{}\verb@  # denominator @\\
\mbox{}\verb@  pp_denom <- tapply(pp_num, list(i=Ycomb[,"i"]), sum, simplify=TRUE)@\\
\mbox{}\verb@  pp <- c(pp_num / pp_denom[Ycomb[,"i"]])@\\
\mbox{}\verb@  if (!is.null(weights)) pp <- weights2 * pp@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Maximization step}

Equation \eqref{E:ExpectedlogLikelihoodCompleteMapFromObs} can be interpreted as the log-likelihood for the SPGLM with fixed cluster size $N$ for an expanded data set: each observation with cluster size $N_i$ and number of responses $y_i$ is expanded to $(N+1)$ replicates with $0, 1, \ldots, N$, responses, respectively. The probabilities $p_{N_iy_is_i}^{(k)}$ serve as cluster weights in the expanded data set. From here we can use a slight generalization of the Newton-Raphson algorithm developed by Wurm and Rathouz (2018) to estimate the baseline density distribution $ \boldsymbol{q}_N^{(0)} $ and the regression coefficients $\boldsymbol{\beta}$. 

Default settings are used for the algorithm, except the starting values is updated based on the previous iteration

\begin{flushleft} \small\label{scrap10}\raggedright\small
\NWtarget{nuweb9b}{} $\langle\,${\itshape M-step}\nobreak\ {\footnotesize {9b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  gldrmControl0 <- gldrm.control(returnfTiltMatrix = TRUE, returnf0ScoreInfo = FALSE, print=FALSE,@\\
\mbox{}\verb@                                betaStart = betasPre, f0Start = referencef0Pre[spt+1])@\\
\mbox{}\verb@@\\
\mbox{}\verb@  mod <- gldrmFit(x = mm2, y=Ycomb[,"y"]/N, linkfun=link$linkfun, linkinv = link$linkinv,@\\
\mbox{}\verb@                  mu.eta = link$mu.eta, mu0 = mu0, offset = offset2, weights = pp, @\\
\mbox{}\verb@                  gldrmControl = gldrmControl0,  thetaControl=theta.control(),@\\
\mbox{}\verb@                  betaControl=beta.control(), f0Control=f0.control())@\\
\mbox{}\verb@                  @\\
\mbox{}\verb@  fTiltMatrix[,spt+1] <- mod$fTiltMatrix[obs_start,]@\\
\mbox{}\verb@  betas <- mod$beta@\\
\mbox{}\verb@  referencef0[spt+1] <- mod$f0@\\
\mbox{}\verb@#  spt <- round(mod$spt * N)@\\
\mbox{}\verb@  llik <- c(log(rowSums(fTiltMatrix * hp)) %*% weights)@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Starting values}
We start the regression coefficients are set to 0, so $q^{(0)}_N$ would be the overall estimate under marginally compatibility. The default value of \texttt{mu0} is set to the mean of $y_i/n_i$.

\begin{flushleft} \small\label{scrap11}\raggedright\small
\NWtarget{nuweb10a}{} $\langle\,${\itshape Set starting values}\nobreak\ {\footnotesize {10a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  betas <- NULL@\\
\mbox{}\verb@  pooled <- CorrBin::CBData(data.frame(Trt = "All", NResp = Y[,1], ClusterSize = rowSums(Y), Freq=ceiling(weights)), @\\
\mbox{}\verb@                    trt="Trt", clustersize="ClusterSize", nresp="NResp", freq="Freq")@\\
\mbox{}\verb@  est <- CorrBin::mc.est(pooled)@\\
\mbox{}\verb@  referencef0 <- est$Prob[est$ClusterSize == N]@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  @\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
Values of $y$ that cannot be a possible number of responses in a cluster of size $N$ corresponding to an observed cluster will always have $q_{y,N}=0$, so they need to be removed from the support during the `gldrm` estimation process (which will use $\log q_{y,N}$)

\begin{flushleft} \small\label{scrap12}\raggedright\small
\NWtarget{nuweb10b}{} $\langle\,${\itshape Identify zero support}\nobreak\ {\footnotesize {10b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # identify possible y-values@\\
\mbox{}\verb@  spt <- sort(unique(Ycomb[,"y"]))@\\
\mbox{}\verb@  data_object$spt <- spt@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # ensure all positive values within support@\\
\mbox{}\verb@  referencef0[spt+1] <- referencef0[spt+1] + 1e-6@\\
\mbox{}\verb@  referencef0 <- referencef0 / sum(referencef0)@\\
\mbox{}\verb@@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  fTiltMatrix <- matrix(rep(referencef0, times=nobs), byrow=TRUE, @\\
\mbox{}\verb@      nrow=nobs, ncol=N+1)@\\
\mbox{}\verb@@\\
\mbox{}\verb@  if (is.null(mu0))@\\
\mbox{}\verb@    mu0 <- weighted.mean(Y[,1]/rowSums(Y), weights)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # hypergeometric terms for log-likelihood calculation@\\
\mbox{}\verb@  hp <- sapply(0:N, function(t)dhyper(x=Y[,1], m=rowSums(Y), n=N-rowSums(Y), k=t))   @\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # initial log-likelihood@\\
\mbox{}\verb@  llik <- log(rowSums(fTiltMatrix * hp)) %*% weights@\\
\mbox{}\verb@@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Standard errors}

We will use numeric derivation to obtain the hessian of the observed data likelihood. The variance-covariance matrix can be estimated as the inverse of the negative hessian. The sum-to-one and fixed-mean constraints of the reference distribution are included by expanding the hessian to a bordered hessian before inversion by including the derivatives of the constraints (this can be derived based on the Lagrange multiplier method). The reference distribution is included in the hessian on the log-scale, and is multiplied by the gradient before inversion to revert to the original scale.

\begin{flushleft} \small\label{scrap13}\raggedright\small
\NWtarget{nuweb11}{} $\langle\,${\itshape Calculate standard errors}\nobreak\ {\footnotesize {11}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  ll <- function(x){@\\
\mbox{}\verb@    spglm_loglik(beta=x[1:p], f0 = exp(x[-(1:p)]), data_object=data_object, link=link)@\\
\mbox{}\verb@  }@\\
\mbox{}\verb@@\\
\mbox{}\verb@  hess <- numDeriv::hessian(func=ll,  x=c(betas, log(referencef0)))@\\
\mbox{}\verb@  hess_idx <- c(1:p, p+spt+1) # betas and f0 elements with non-zero support@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # revert to unlogged f0@\\
\mbox{}\verb@  grad <- c(rep(1, p), 1/referencef0[spt+1])@\\
\mbox{}\verb@  hess1 <- diag(grad) %*% hess[hess_idx, hess_idx] %*% diag(grad)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # create bordered hessian@\\
\mbox{}\verb@  border1 <- c(rep(0, p), rep(1, length(spt)))  # gradient of sum-to-one constraint@\\
\mbox{}\verb@  border2 <- c(rep(0, p), spt)          # gradient of fixed-mean constraint@\\
\mbox{}\verb@  bhess <- rbind(cbind(hess1, border1, border2), c(border1,0,0), c(border2,0,0))@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # calculate variance-covariance matrix@\\
\mbox{}\verb@  bvc <- solve(-bhess)@\\
\mbox{}\verb@  # remove border and set to 0 for zero-support values@\\
\mbox{}\verb@  vc <- matrix(0, nrow=p+N+1, ncol=p+N+1)@\\
\mbox{}\verb@  vc[hess_idx, hess_idx] <- bvc[1:(p+length(spt)), 1:(p+length(spt))]     @\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb7}{7}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\section{Simulate data from semi-parametric model}

\begin{flushleft} \small\label{scrap14}\raggedright\small
\NWtarget{nuweb?}{} \verb@"../R/SPGLM.R"@\nobreak\ {\footnotesize {?}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@#' Simulate data that follows an SPGLM @\\
\mbox{}\verb@#'@\\
\mbox{}\verb@#' @\\
\mbox{}\verb@#' @{\tt @}\verb@param n vector of cluster-sizes for the simulated data.@\\
\mbox{}\verb@#' @{\tt @}\verb@param means vector of mean event probability for each cluster. Values should be between 0 and 1. @\\
\mbox{}\verb@#' @{\tt @}\verb@param q0 the backbone probability distribution of number of events in a cluster of maximal size. @\\
\mbox{}\verb@#' @{\tt @}\verb@return a data frame with columns `Mean`, `ClusterSize`, `NResp`@\\
\mbox{}\verb@#' @{\tt @}\verb@export@\\
\mbox{}\verb@#' @{\tt @}\verb@importFrom stats rhyper@\\
\mbox{}\verb@@\\
\mbox{}\verb@ran.spglm <- function(n, means, q0){@\\
\mbox{}\verb@@\\
\mbox{}\verb@  if (length(n) != length(means)) stop("`n` and `means` should have the same length")@\\
\mbox{}\verb@  if (any(means < 0) || any(means > 1)) stop("All values of `means` should be between 0 and 1")@\\
\mbox{}\verb@  if (any(q0 < 0)) stop("All values of `q0` should be non-negative")@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  q0 <- q0 / sum(q0)@\\
\mbox{}\verb@  N <- length(q0) - 1@\\
\mbox{}\verb@  spt <- which(q0 > 0) - 1@\\
\mbox{}\verb@  nobs <- length(n)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # get event probabilities at max clustersize for each cluster@\\
\mbox{}\verb@  th <- getTheta(spt = spt/N, f0 = q0[spt+1], mu = means, @\\
\mbox{}\verb@                 weights = rep(1, nobs), ySptIndex = rep(1, nobs))@\\
\mbox{}\verb@                 @\\
\mbox{}\verb@  probmat <- t(th$fTilt)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # random number of events at max clustersize@\\
\mbox{}\verb@  nevents <- apply(probmat, 1, function(pvec)sample(x=spt, size=1, replace=FALSE, prob=pvec))@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # random number of events in actual clusters using marginal compatibility@\\
\mbox{}\verb@  nresp <- rhyper(nobs, m=nevents, n=N-nevents, k=n)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  res <- data.frame(Mean = means, ClusterSize = n, NResp = nresp)@\\
\mbox{}\verb@  res@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtFileDefBy\ \NWlink{nuweb1}{1}\NWlink{nuweb3}{, 3}\NWlink{nuweb4}{, 4}\NWlink{nuweb5}{, 5}\NWlink{nuweb6}{, 6}\NWlink{nuweb?}{, ?}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\end{document}
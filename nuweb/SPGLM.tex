\newcommand{\NWtarget}[2]{#2}
\newcommand{\NWlink}[2]{#2}
\newcommand{\NWtxtMacroDefBy}{Fragment defined by}
\newcommand{\NWtxtMacroRefIn}{Fragment referenced in}
\newcommand{\NWtxtMacroNoRef}{Fragment never referenced}
\newcommand{\NWtxtDefBy}{Defined by}
\newcommand{\NWtxtRefIn}{Referenced in}
\newcommand{\NWtxtNoRef}{Not referenced}
\newcommand{\NWtxtFileDefBy}{File defined by}
\newcommand{\NWtxtIdentsUsed}{Uses:}
\newcommand{\NWtxtIdentsNotUsed}{Never used}
\newcommand{\NWtxtIdentsDefed}{Defines:}
\newcommand{\NWsep}{${\diamond}$}
\newcommand{\NWnotglobal}{(not defined globally)}
\newcommand{\NWuseHyperlinks}{}
\documentclass[reqno]{amsart}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\renewcommand{\NWtarget}[2]{\hypertarget{#1}{#2}}
\renewcommand{\NWlink}[2]{\hyperlink{#1}{#2}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bpi}{\text{\boldmath $\pi$}}
\newcommand{\leqst}{\mathrel{\preceq^{st}}}
\newcommand{\geqst}{\mathrel{\succeq^{st}}}
\newcommand{\qz}{\boldsymbol{q}^{(0)}}
\DeclareMathOperator{\E}{\mathbb{E}}

\usepackage{dsfont}



\title{Semi-parametric generalized linear model for correlated binary data}
\author{Aniko Szabo}
\date{\today}
\begin{document}
\maketitle

Note: this setup assumes that parts defined in \texttt{SPregress.w} are available.

\section{Introduction}

We extend the Rathouz-Gao semi-parametric generalized linear model to clustered binary outcomes with varying cluster sizes. Treating the number of events $Y_i$ from a cluster scaled by corresponding cluster size $n_i$ as the response variable, we use a parametric regression model for the marginal response probability $Y_/n_i$ and assume that for the maximum cluster size $N$ the probability density distribution of $Y_i$ is an exponentially tilted version of the reference density distribution.

The conditional mean model is as follows:
\begin{equation}\label{M:conditionalMeanSPGLM}
     E(\dfrac{Y_i}{n_i} | \boldsymbol{Z}_i; \boldsymbol{\beta})=\mu(\boldsymbol{Z}_i,\boldsymbol{\beta}) \equiv \mu_i = h^{-1}( \boldsymbol{Z}_i^{T} \boldsymbol{\beta})
\end{equation}
where $\boldsymbol{Z}_i$ is a matrix of cluster-level covariates, the function $h(\cdot)$ is a known strictly increasing link function.

The assumed probability density function for a cluster of maximal size $N$ is:
\begin{equation}\label{M:densitySPGLM}
    q_{y,N} (\boldsymbol{Z}, \boldsymbol{\beta}) \propto q_{y,N}^{(0)} \times \exp [ - \omega(\boldsymbol{Z}, \boldsymbol{\beta}) \times y]
\end{equation}
where $q_{y,N}^{(0)}$ is a non-parametric reference distribution, and $\omega(\boldsymbol{Z}, \boldsymbol{\beta})$ is the tilting parameter chosen so that the expectation of $Y/N$ equals to the value defined by the mean model \eqref{M:conditionalMeanSPGLM}, i.e.\  $\sum_{y=0}^{N} \dfrac{y}{N} q_{y,N}(\boldsymbol{Z}, \boldsymbol{\beta}) = \mu = h^{-1}( \boldsymbol{Z}^{T} \boldsymbol{\beta}) $. 
%We define the tilting parameter for the reference density to be $0$, i.e., $\omega_0 \equiv 0$.

For the purpose of identifiability, the mean of marginal probability $\{ q_{y,N}^{(0)} \}$ is set at an arbitrary fixed value, which can be the mean of the reference group, or the overall marginal mean of the data. We extend the conditional mean and probability density model for  cluster sizes smaller than the maximal size $N$ by assuming marginal compatibility.

\section{Main function}

\begin{flushleft} \small\label{scrap1}\raggedright\small
\NWtarget{nuweb1}{} \verb@"../R/SPGLM.R"@\nobreak\ {\footnotesize {1}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@#' Fit semi-parametric GLM@\\
\mbox{}\verb@#'@\\
\mbox{}\verb@#'@{\tt @}\verb@rdname spglm@\\
\mbox{}\verb@#'@{\tt @}\verb@param formula a one-sided formula of the form \code{cbind(r, s) ~ predictors} where \code{r} and \code{s} give the number of responses and non-responses within each cluster, respectively (so the cluster size is \code{r+s}), and \code{predictors} describes the covariates.@\\
\mbox{}\verb@#'@{\tt @}\verb@param data  an optional matrix or data frame containing the variables in the formula \code{formula}. By default the variables are taken from \code{environment(formula).}@\\
\mbox{}\verb@#'@{\tt @}\verb@param subset  an optional vector specifying a subset of observations to be used.@\\
\mbox{}\verb@#'@{\tt @}\verb@param weight  an optional vector specifying observation weights.@\\
\mbox{}\verb@#'@{\tt @}\verb@param link      a link function for the mean.@\\
\mbox{}\verb@#'@{\tt @}\verb@param mu0       an optional numeric value constraining the mean of the baseline distribution@\\
\mbox{}\verb@#'@{\tt @}\verb@param control a list with parameters controlling the algorithm.@\\
\mbox{}\verb@#'@{\tt @}\verb@return an object of class \code{spglm} with the fitted model.@\\
\mbox{}\verb@#'@{\tt @}\verb@export@\\
\mbox{}\verb@#' @{\tt @}\verb@importFrom stats terms model.matrix@\\
\mbox{}\verb@@\\
\mbox{}\verb@spglm <- function(formula, data, subset, weights, offset, link="logit", mu0=NULL, @\\
\mbox{}\verb@                  control=list(eps=0.001, maxit=100), ...){@\\
\mbox{}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Create model matrix from formula and data}\nobreak\ {\footnotesize \NWlink{nuweb2}{2}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    data_object <- list(model_matrix=mm, resp=Y, n=rowSums(Y), weights=weights, offset=offset)@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape Fit model}\nobreak\ {\footnotesize \NWlink{nuweb3b}{3b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@@\\
\mbox{}\verb@    mt <- attr(mf, "terms")@\\
\mbox{}\verb@    res <- list(coefficients = betas, SE = SEbeta, f0=referencef0, SEf0 = SEf0, mu0=mu0, niter = iter, loglik=llik,@\\
\mbox{}\verb@                link = link, call = mc, terms = mt,@\\
\mbox{}\verb@                xlevels = .getXlevels(mt, mf),@\\
\mbox{}\verb@                data_object=data_object)@\\
\mbox{}\verb@    class(res) <- "spglm"@\\
\mbox{}\verb@    res@\\
\mbox{}\verb@@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtFileDefBy\ \NWlink{nuweb1}{1}\NWlink{nuweb3a}{, 3a}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\begin{flushleft} \small\label{scrap2}\raggedright\small
\NWtarget{nuweb2}{} $\langle\,${\itshape Create model matrix from formula and data}\nobreak\ {\footnotesize {2}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@   if (missing(formula) || (length(formula) != 3L))@\\
\mbox{}\verb@        stop("'formula' missing or incorrect")@\\
\mbox{}\verb@   if (missing(data))@\\
\mbox{}\verb@        data <- environment(formula)@\\
\mbox{}\verb@    mc <- match.call(expand.dots = FALSE)@\\
\mbox{}\verb@    m <- match(c("formula", "data", "subset", "weights", "offset"), names(mc), 0L)@\\
\mbox{}\verb@    m <- mc[c(1L,m)]@\\
\mbox{}\verb@    if (is.matrix(eval(m$data, parent.frame())))@\\
\mbox{}\verb@        m$data <- as.data.frame(data)@\\
\mbox{}\verb@    m[[1L]] <- quote(stats::model.frame)@\\
\mbox{}\verb@    mf <- eval(m, parent.frame())@\\
\mbox{}\verb@@\\
\mbox{}\verb@    # extract and check response variable@\\
\mbox{}\verb@    Y <- model.response(mf)@\\
\mbox{}\verb@    if (ncol(Y) != 2) stop("The response variable should be a 2-column matrix")@\\
\mbox{}\verb@@\\
\mbox{}\verb@    # create model matrix@\\
\mbox{}\verb@    mm <- model.matrix(formula, data=mf)@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    if (is.character(link)) {@\\
\mbox{}\verb@      link <- stats::make.link(link)@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@    else if (!is.list(link) || !(all(c("linkfun", "linkinv", @\\
\mbox{}\verb@                                     "mu.eta") %in% names(link)))) {@\\
\mbox{}\verb@      stop(paste0("link should be a character string or a list containing ", @\\
\mbox{}\verb@                "functions named linkfun, linkinv, and mu.eta"))@\\
\mbox{}\verb@    }@\\
\mbox{}\verb@@\\
\mbox{}\verb@    # extract offset@\\
\mbox{}\verb@    offset <- as.vector(model.offset(mf))@\\
\mbox{}\verb@    if (is.null(offset)) @\\
\mbox{}\verb@      offset <- rep(0, nrow(mm))@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    # extract weights@\\
\mbox{}\verb@    weights <- as.vector(model.weights(mf))@\\
\mbox{}\verb@    if (!is.null(weights) && !is.numeric(weights))@\\
\mbox{}\verb@        stop("'weights' must be a numeric vector")@\\
\mbox{}\verb@    if (!is.null(weights) && any(weights < 0))@\\
\mbox{}\verb@        stop("negative weights not allowed")@\\
\mbox{}\verb@     if (is.null(weights))@\\
\mbox{}\verb@        weights <- rep(1, nrow(mm))@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb1}{1}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\section{EM algorithm for model fitting}
We will combine an Expectation Maximization with the Newton-Raphson algorithm proposed by Rathouz and Gao (2009) to estimate the model parameters. Recall that the observed data consists of the number of events, cluster size, and cluster-level covariates: $(y_i, N_i, \boldsymbol{Z}_i)$, $i=1,\cdots,I$. 

\subsection{Observed data log-likelihood}
Under the marginal compatibility assumption, the observed log-likelihood function is:
\begin{equation}\label{E:loglikelihood}
\begin{split}
    \ell (q_{y,N}, \boldsymbol{\beta}) &= \sum_{i=1}^I \log \{ q_{y_i,N_i} (\boldsymbol{Z}_i, \boldsymbol{\beta}) \} \\
    &= \sum_{i=1}^I \log \bigg[ \binom{N_i}{y_i} \sum_{t=y}^{N-N_i+y_i} \dfrac{\binom{N-N_i}{t-y_i}}{\binom{N}{t}} \times \dfrac{q_{t,N}^{(0)} \times \exp \{ \omega(\boldsymbol{Z}_i; \boldsymbol{\beta}) t \}} {\sum_{t'=0}^{N} q_{t',N}^{(0)} \times \exp \{ \omega(\boldsymbol{Z}_i; \boldsymbol{\beta}) t' \}}  \bigg].
\end{split}
\end{equation}

\subsection{Model prediction and likelihood}
We define internal function to calculate predicted values and the log-likelihood.

\begin{flushleft} \small\label{scrap3}\raggedright\small
\NWtarget{nuweb3a}{} \verb@"../R/SPGLM.R"@\nobreak\ {\footnotesize {3a}}$\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@spglm_pred_mean <- function(beta, data_object, link){@\\
\mbox{}\verb@  eta <- c(data_object$model_matrix %*% beta + data_object$offset)@\\
\mbox{}\verb@  mu <- link$linkinv(eta)@\\
\mbox{}\verb@  mu@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@\\
\mbox{}\verb@spglm_loglik <- function(beta, f0, data_object, link){@\\
\mbox{}\verb@  @\\
\mbox{}\verb@    mu <- spglm_pred_mean(beta=beta, data_object=data_object, link=link)@\\
\mbox{}\verb@    N <- max(data_object$n)@\\
\mbox{}\verb@    nobs <- nrow(data_object$model_matrix)@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    # ySptIndex is only used to calculate the log-likelihood, which we will not be using@\\
\mbox{}\verb@    th <- getTheta(spt=(0:N)/N, f0=f0, mu=mu, weights=data_object$weights, ySptIndex=rep(1, nobs),@\\
\mbox{}\verb@                   thetaStart=NULL, thetaControl=theta.control())@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    hp <- sapply(0:N, function(t)dhyper(x=data_object$resp[,1], m=data_object$n, n=N-data_object$n, k=t))   @\\
\mbox{}\verb@    llik_term <- log(rowSums(t(th$fTilt) * hp))@\\
\mbox{}\verb@    llik <- llik_term %*% data_object$weights@\\
\mbox{}\verb@    c(llik)@\\
\mbox{}\verb@}@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtFileDefBy\ \NWlink{nuweb1}{1}\NWlink{nuweb3a}{, 3a}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
It is difficult to utilize the observed log-likelihood directly for parameter estimation. Its score function is complex because there is a logarithm of a sum in \eqref{E:loglikelihood}. Therefore, we implement an EM algorithm for parameter estimation.


\begin{flushleft} \small\label{scrap4}\raggedright\small
\NWtarget{nuweb3b}{} $\langle\,${\itshape Fit model}\nobreak\ {\footnotesize {3b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@ @\hbox{$\langle\,${\itshape Set starting values}\nobreak\ {\footnotesize \NWlink{nuweb6b}{6b}}$\,\rangle$}\verb@@\\
\mbox{}\verb@ @\hbox{$\langle\,${\itshape Set up for E-step}\nobreak\ {\footnotesize \NWlink{nuweb4}{4}}$\,\rangle$}\verb@@\\
\mbox{}\verb@ @\\
\mbox{}\verb@ iter <- 0@\\
\mbox{}\verb@ difference <- 100@\\
\mbox{}\verb@ while (iter < control$maxit & difference > control$eps) {@\\
\mbox{}\verb@    iter <- iter + 1@\\
\mbox{}\verb@    referencef0Pre <- referencef0@\\
\mbox{}\verb@    betasPre <- betas@\\
\mbox{}\verb@    llikPre <- llik@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape E-step}\nobreak\ {\footnotesize \NWlink{nuweb5}{5}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\hbox{$\langle\,${\itshape M-step}\nobreak\ {\footnotesize \NWlink{nuweb6a}{6a}}$\,\rangle$}\verb@@\\
\mbox{}\verb@    @\\
\mbox{}\verb@    difference <- abs(llik - llikPre)@\\
\mbox{}\verb@  }@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  @\hbox{$\langle\,${\itshape Calculate standard errors}\nobreak\ {\footnotesize \NWlink{nuweb?}{?}}$\,\rangle$}\verb@@\\
\mbox{}\verb@  @\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb1}{1}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Missing data setup}
Stefanescu et al (2003) have proved that the marginal compatibility assumption is equivalent to assuming that each cluster of size $N'$ arises from a cluster of maximal cluster size $N$ with some binary observations missing completely missing at random (MCAR). The missing data setup and the expectation step from EM algorithm are based on this MCAR interpretation. 

For the setup of the EM algorithm, the complete data are clusters of size $N$ with a corresponding number of events $s_i$, where $s_i \in \{y_i, \cdots, N \}$. The missing data correspond to the unobserved outcomes in of the $(N - N_i)$ cluster components. The missing data can be summarized as $(s_i - y_i)$ events out of $(N - N_i)$ elements.

The complete data log-likelihood is:
\begin{equation}\label{E:logLikelihoodCompleteMapFromObs}
\begin{split}
    \ell_{\text{complete}} (q_{y,N}, \boldsymbol{\beta} \mid N_i,s_i) &= \sum_{i=0}^I \log \{ q_{s_i,N}(\boldsymbol{Z}_i; \boldsymbol{\beta}) \} \\
    &= \sum_{i=0}^I \sum_{s=y_i}^N {\mathds{1}}(s_i=s) \times \log \{ q_{s,N}(\boldsymbol{Z}_i; \boldsymbol{\beta}) \}, 
\end{split}
\end{equation}
where $q_{s_i,N}$ is the probability of achieving $s_i$ events in a cluster of size $N$ in the complete data set. The $\mathds{1}$ denotes the indicator function.


Equation \ref{E:logLikelihoodCompleteMapFromObs} can be interpreted as the log-likelihood of a model with fixed cluster size $N$ for an expanded data set: each observation with cluster size $N_i$ and number of responses $y_i$ is expanded to $(N+1)$ replicates with $0, 1, \ldots, N$, responses, respectively.

\begin{flushleft} \small\label{scrap5}\raggedright\small
\NWtarget{nuweb4}{} $\langle\,${\itshape Set up for E-step}\nobreak\ {\footnotesize {4}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  # replace each cluster with N+1 clusters of size N@\\
\mbox{}\verb@  new <- cbind(y=0:N)@\\
\mbox{}\verb@  rep_idx <- rep(1:nrow(Y), each=nrow(new))@\\
\mbox{}\verb@  Y2 <- cbind(1:nrow(Y), Y)[rep_idx,]@\\
\mbox{}\verb@  colnames(Y2) <- c("i", "resp","nonresp")@\\
\mbox{}\verb@@\\
\mbox{}\verb@  rep_idx2 <- rep(1:nrow(new), times=nrow(Y))@\\
\mbox{}\verb@  Ycomb <- cbind(Y2, new[rep_idx2, ,drop=FALSE])@\\
\mbox{}\verb@  # select possible combinations@\\
\mbox{}\verb@  possible <- (Ycomb[,"y"] >= Ycomb[,"resp"]) & (N-Ycomb[,"y"] >= Ycomb[,"nonresp"])@\\
\mbox{}\verb@  Ycomb <- Ycomb[possible,]@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  obs_start <- match(1:nobs, Ycomb[,"i"])@\\
\mbox{}\verb@ @\\
\mbox{}\verb@  mm2 <- mm[Ycomb[,"i"], ,drop=FALSE]@\\
\mbox{}\verb@  weights2 <- weights[Ycomb[,"i"]]@\\
\mbox{}\verb@  offset2 <- offset[Ycomb[,"i"]]@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb3b}{3b}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Expectation step}

We take the expectation of complete data log-likelihood \eqref{E:logLikelihoodCompleteMapFromObs} given parameters from the previous $k^{th}$ iteration, which is shown as follows:
\begin{equation}\label{E:ExpectedlogLikelihoodCompleteMapFromObs}
\begin{split}
    Q \bigg( q_{y,N}^{(k)}, \boldsymbol{\beta}^{(k)} \bigg) &= 
     \E \bigg\{ \ell_{\text{complete}} (q_{y,N}, \boldsymbol{\beta} \mid N_i, s_i) \mid q_{y,N}^{(k)}, \boldsymbol{\beta}^{(k)}, N_i, y_i \bigg\} \\
    &= \E \Bigg[ \sum_{i=0}^I \sum_{s=0}^N {\mathds{1}}(s=s_i) \times \log \{ q_{s,N}(\boldsymbol{Z}_i; \boldsymbol{\beta}) \} \mid q_{y,N}^{(k)}, y_i,N_i \Bigg] \\
    &= \sum_{i=0}^I \sum_{s=0}^N \underbrace{ \E \big\{ {\mathds{1}}(s=s_i) \mid q_{y,N}^{(k)}, \boldsymbol{\beta}^{(k)}, y_i, N_i \big\}}_{p_{N_iy_is_i}^{(k)}} \times \log \{q_{s,N}(\boldsymbol{Z}_i; \boldsymbol{\beta}) \},
\end{split}
\end{equation}
where $p_{N'y'y} = \textbf{Pr}_N(Y=y \mid Y'=y',N')$ denotes the conditional probability of achieving $y$ events in the complete cluster of size $N$, given that $y'$ events have been observed in the original cluster of size $N'$. 
The expression for $p_{N'y'y}$ can be derived from $q_{y,N}$ using Bayes theorem and the MCAR interpretation as follows:
\begin{equation}\label{D:prst}
\begin{split}
p_{N'y'y} &= \textbf{Pr}_N(Y=y \mid Y'=y',N') \\ 
          &= \frac{\textbf{Pr}_{N'}(Y'=y' \mid Y=y) \textbf{Pr}_N(Y=y)}{\sum_{t=0}^{N} \textbf{Pr}_{y'}(Y'=y' \mid Y=t) \textbf{Pr}_N(Y=t)} \\ 
          &= \frac{\binom{y}{y'} \binom{N-y}{N'-y'} q_{y,N}}{\sum_{t=y'}^{N-N'+y'} \dbinom{t}{y'} \binom{N-t}{N'-y'} q_{t,N}},
\end{split}
\end{equation}
and $p_{N_iy_is_i}^{(k)}$ at the $k$th step of the iteration can be obtained by using $q_{y,N}^{(k)}$ on the right-hand side of the expression. 

The value of $q_{y,N}^{(k)}((\boldsymbol{Z}_i; \boldsymbol{\beta}^{(k)})$ can be obtained from ${q_{y,N}^{(0)}}^{(k)}$ using \ref{M:densitySPGLM}. But it is also returned by \texttt{gldrmFit} as \texttt{fTiltMatrix}.


\begin{flushleft} \small\label{scrap6}\raggedright\small
\NWtarget{nuweb5}{} $\langle\,${\itshape E-step}\nobreak\ {\footnotesize {5}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  # convert fTiltMatrix to long vector, watching out for potentially different support@\\
\mbox{}\verb@  y_idx <- match(Ycomb[,"y"], spt)@\\
\mbox{}\verb@  fTilt2 <- ifelse(!is.na(y_idx), fTiltMatrix[cbind(Ycomb[,"i"], y_idx)], 0)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # numerator of weights@\\
\mbox{}\verb@  pp_num <- choose(n=Ycomb[,"y"], k=Ycomb[,"resp"]) * @\\
\mbox{}\verb@           choose(n=N-Ycomb[,"y"], k=Ycomb[,"nonresp"]) * @\\
\mbox{}\verb@           fTilt2@\\
\mbox{}\verb@  # denominator @\\
\mbox{}\verb@  pp_denom <- tapply(pp_num, list(i=Ycomb[,"i"]), sum, simplify=TRUE)@\\
\mbox{}\verb@  pp <- c(pp_num / pp_denom[Ycomb[,"i"]])@\\
\mbox{}\verb@  if (!is.null(weights)) pp <- weights2 * pp@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb3b}{3b}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Maximization step}

Equation \eqref{E:ExpectedlogLikelihoodCompleteMapFromObs} can be interpreted as the log-likelihood for the SPGLM with fixed cluster size $N$ for an expanded data set: each observation with cluster size $N_i$ and number of responses $y_i$ is expanded to $(N+1)$ replicates with $0, 1, \ldots, N$, responses, respectively. The probabilities $p_{N_iy_is_i}^{(k)}$ serve as cluster weights in the expanded data set. From here we can use a slight generalization of the Newton-Raphson algorithm developed by Wurm and Rathouz (2018) to estimate the baseline density distribution $ \boldsymbol{q}_N^{(0)} $ and the regression coefficients $\boldsymbol{\beta}$. 

Default settings are used for the algorithm, except the starting values is updated based on the previous iteration

\begin{flushleft} \small\label{scrap7}\raggedright\small
\NWtarget{nuweb6a}{} $\langle\,${\itshape M-step}\nobreak\ {\footnotesize {6a}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  gldrmControl0 <- gldrm.control(returnfTiltMatrix = TRUE, returnf0ScoreInfo = FALSE, print=FALSE,@\\
\mbox{}\verb@                                betaStart = betasPre, f0Start = referencef0Pre)@\\
\mbox{}\verb@@\\
\mbox{}\verb@  mod <- gldrmFit(x = mm2, y=Ycomb[,"y"]/N, linkfun=link$linkfun, linkinv = link$linkinv,@\\
\mbox{}\verb@                  mu.eta = link$mu.eta, mu0 = mu0, offset = offset2, weights = pp, @\\
\mbox{}\verb@                  gldrmControl = gldrmControl0,  thetaControl=theta.control(),@\\
\mbox{}\verb@                  betaControl=beta.control(), f0Control=f0.control())@\\
\mbox{}\verb@                  @\\
\mbox{}\verb@  fTiltMatrix <- mod$fTiltMatrix[obs_start,]@\\
\mbox{}\verb@  betas <- mod$beta@\\
\mbox{}\verb@  referencef0 <- mod$f0@\\
\mbox{}\verb@  spt <- round(mod$spt * N)@\\
\mbox{}\verb@  llik <- c(log(rowSums(fTiltMatrix * hp)) %*% weights)@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb3b}{3b}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Starting values}
We start the regression coefficients are set to 0, so $q^{(0)}_N$ would be the overall estimate under marginally compatibility. The default value of \texttt{mu0} is set to the mean of $y_i/n_i$.

\begin{flushleft} \small\label{scrap8}\raggedright\small
\NWtarget{nuweb6b}{} $\langle\,${\itshape Set starting values}\nobreak\ {\footnotesize {6b}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  N <- max(rowSums(Y))@\\
\mbox{}\verb@  nobs <- nrow(mm)@\\
\mbox{}\verb@  p <- ncol(mm)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  betas <- NULL@\\
\mbox{}\verb@  pooled <- CBData(data.frame(Trt = "All", NResp = Y[,1], ClusterSize = rowSums(Y), Freq=ceiling(weights)), @\\
\mbox{}\verb@                    trt="Trt", clustersize="ClusterSize", nresp="NResp", freq="Freq")@\\
\mbox{}\verb@  est <- mc.est(pooled)@\\
\mbox{}\verb@  referencef0 <- est$Prob[est$ClusterSize == N]@\\
\mbox{}\verb@  # ensure all positive values@\\
\mbox{}\verb@  referencef0 <- (referencef0 + 1e-6)/(1+(N+1)*1e-6)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  fTiltMatrix <- matrix(rep(referencef0, times=nobs), byrow=TRUE, nrow=nobs, ncol=N+1)@\\
\mbox{}\verb@  spt <- 0:N@\\
\mbox{}\verb@@\\
\mbox{}\verb@  if (is.null(mu0))@\\
\mbox{}\verb@    mu0 <- weighted.mean(Y[,1]/rowSums(Y), weights)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # hypergeometric terms for log-likelihood calculation@\\
\mbox{}\verb@  hp <- sapply(0:N, function(t)dhyper(x=Y[,1], m=rowSums(Y), n=N-rowSums(Y), k=t))   @\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # initial log-likelihood@\\
\mbox{}\verb@  llik <- log(rowSums(fTiltMatrix * hp)) %*% weights@\\
\mbox{}\verb@@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb3b}{3b}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\subsection{Standard errors}

We will use numeric derivation to obtain the hessian of the observed data likelihood. The variance-covariance matrix can be estimated as the inverse of the negative hessian. The sum-to-one and fixed-mean constraints of the reference distribution are included by expanding the hessian to a bordered hessian before inversion by including the derivatives of the constraints (this can be derived based on the Lagrange multiplier method). The reference distribution is included in the hessian on the log-scale, and is multiplied by the gradient before inversion to revert to the original scale.

\begin{flushleft} \small\label{scrap9}\raggedright\small
\NWtarget{nuweb?}{} $\langle\,${\itshape Calculate standard errors}\nobreak\ {\footnotesize {?}}$\,\rangle\equiv$
\vspace{-1ex}
\begin{list}{}{} \item
\mbox{}\verb@@\\
\mbox{}\verb@  ll <- function(x){@\\
\mbox{}\verb@    spglm_loglik(beta=x[1:p], f0 = exp(x[-(1:p)]), data_object=data_object, link=link)@\\
\mbox{}\verb@  }@\\
\mbox{}\verb@@\\
\mbox{}\verb@  hess <- numDeriv::hessian(func=ll,  x=c(betas, log(referencef0)))@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # revert to unlogged f0@\\
\mbox{}\verb@  grad <- c(rep(1, p), 1/referencef0)@\\
\mbox{}\verb@  hess1 <- diag(grad) %*% hess %*% diag(grad)@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # create bordered hessian@\\
\mbox{}\verb@  border1 <- c(rep(0, p), rep(1, N+1))  # gradient of sum-to-one constraint@\\
\mbox{}\verb@  border2 <- c(rep(0, p), 0:N)          # gradient of fixed-mean constraint@\\
\mbox{}\verb@  bhess <- rbind(cbind(hess1, border1, border2), c(border1,0,0), c(border2,0,0))@\\
\mbox{}\verb@  @\\
\mbox{}\verb@  # calculate variance-covariance matrix@\\
\mbox{}\verb@  vc <- solve(-bhess)@\\
\mbox{}\verb@  SEbeta <- sqrt(diag(vc)[1:p])@\\
\mbox{}\verb@  SEf0 <- sqrt(diag(vc)[p+1+(0:N)])@\\
\mbox{}\verb@@{\NWsep}
\end{list}
\vspace{-1.5ex}
\footnotesize
\begin{list}{}{\setlength{\itemsep}{-\parsep}\setlength{\itemindent}{-\leftmargin}}
\item \NWtxtMacroRefIn\ \NWlink{nuweb3b}{3b}.

\item{}
\end{list}
\vspace{4ex}
\end{flushleft}
\end{document}